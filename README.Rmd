---
output: rmarkdown::github_document
editor_options: 
  chunk_output_type: console
---

`newsflash` : Tools to Work with the Internet Archive and GDELT Television Explorer

Ref: 

- <http://television.gdeltproject.org/cgi-bin/iatv_ftxtsearch/iatv_ftxtsearch>
- <https://archive.org/details/third-eye>

TV Explorer:
>_"In collaboration with the Internet Archive's Television News Archive, GDELT's Television Explorer allows you to keyword search the closed captioning streams of the Archive's 6 years of American television news and explore macro-level trends in how America's television news is shaping the conversation around key societal issues. Unlike the Archive's primary Television News interface, which returns results at the level of an hour or half-hour "show," the interface here reaches inside of those six years of programming and breaks the more than one million shows into individual sentences and counts how many of those sentences contain your keyword of interest. Instead of reporting that CNN had 24 hour-long shows yesterday that mentioned Donald Trump, the interface here will count how many sentences uttered on CNN yesterday mentioned his name - a vastly more accurate metric for assessing media attention."_

Third Eye:
>_The TV News Archive's Third Eye project captures the chyrons‚Äìor narrative text‚Äìthat appear on the lower third of TV news screens and turns them into downloadable data and a Twitter feed for research, journalism, online tools, and other projects. At project launch (September 2017) we are collecting chyrons from BBC News, CNN, Fox News, and MSNBC‚Äìmore than four million collected over just two weeks."_

An advantage of using this over the TV Explorer interactive selector & downloader or Third Eye API is that you get tidy tibbles with this package, ready to use in R.

NOTE: While I don't claim that this alpha-package is anywhere near perfect, the IA/GDELT TV API hiccups every so often so when there are critical errors run the same query in their web interface before submitting an issue. I kept getting errors when searching all affiliate markets for the "mexican president" query that also generate errors on the web site when JSON is selected as output (it's fine on the web site if the choice is interactive browser visualizations). Submit those errors to them, not here.

NOTE: As of 2017-01-29 the API is not returning `keyword_primary` in the `query_details` field. It's not an error with this package.

The following functions are implemented:

- `query_tv`:	Issue a query to the TV Explorer
- `top_text`: Helper function to extract the text snippets from the top matches as a tidytext-compatible tibble or plain character vector
- `list_networks`:	Helper function to identify station/network keyword and corpus date range for said market
- `top_trending`:	Top Trending Tables
- `top_trending_ranged`:	Top Trending Tables (**you should use this one**)
- `print.newsflash`: Helper print method for a nicer default text summary
- `read_chyrons`:	Retrieve TV News Archive chyrons from the Internet Archive's Third Eye project
- `list_chyrons`:	Retrieve Third Eye chyron index
- `view_clip`:	View news segment clips from chyron details frament

### Installation

```{r eval=FALSE}
devtools::install_github("hrbrmstr/newsflash")
```

```{r message=FALSE, warning=FALSE, error=FALSE}
options(width=120)
```

### Usage

```{r message=FALSE, warning=FALSE, error=FALSE}
library(newsflash)
library(ggalt)
library(hrbrthemes)
library(tidyverse)

# current verison
packageVersion("newsflash")
```

"Third Eye" Chyrons are simpler so we'll start with them first:

```{r fig.width=8, fig.height=5}
list_chyrons()

ch <- read_chyrons("2017-09-30")

mutate(
  ch, 
  hour = lubridate::hour(ts),
  text = tolower(text),
  mention = grepl("erto ri", text)
) %>% 
  filter(mention) %>% 
  count(hour, channel) %>% 
  ggplot(aes(hour, n)) +
  geom_segment(aes(xend=hour, yend=0)) +
  scale_x_continuous(name="Hour (GMT)", breaks=seq(0, 23, 6),
                     labels=sprintf("%02d:00", seq(0, 23, 6))) +
  scale_y_continuous(name="# Chyrons", limits=c(0,30)) +
  facet_wrap(~channel, scales="free") +
  labs(title="Chyrons mentioning 'Puerto Rico' per hour per channel",
       caption="Source: Internet Archive Third Eye project & <github.com/hrbrmstr/newsflash>") +
  theme_ipsum_rc(grid="Y")
```

Now for the TV Explorer:

See what networks & associated corpus date ranges are available:

```{r}
list_networks(widget=FALSE)
```

Basic search:

```{r}
query_tv("clinton", "email", "AFFMARKALL")
```

The closed-caption text snippets are returned for the "top matches" (usually max 2,500 for a broad enough search) and you can extract them from the object directly with `x$top_matches$snippet` or use `top_text(x)`:

```{r}
mex <- query_tv("mexican president", filter_network="NATIONAL")
top_text(mex)
```

```{r}
head(top_text(mex, tidy=FALSE))
```

You can, of course, do other things with the various bits of data returned:

```{r}
orange <- query_tv("trump")
```

```{r fig.height=2.75, fig.retina=2}
arrange(orange$station_histogram, value) %>% 
  mutate(station=factor(station, levels=station)) %>% 
  ggplot(aes(value, station)) +
  geom_lollipop(horizontal=TRUE, size=0.75,
                color=ggthemes::tableau_color_pal()(10)[2]) +
  scale_x_continuous(expand=c(0,0), label=scales::comma, limits=c(0,400000)) +
  labs(y=NULL, x="# Mentions",
       title="Station Histogram") +
  theme_ipsum_rc(grid="X")
```

```{r warning=FALSE, message=FALSE, fig.height=6, fig.retina=2}
mutate(orange$timeline, date_start=as.Date(date_start)) %>% 
  filter(date_start >= as.Date("2015-01-01")) %>% 
  ggplot(aes(date_start, value)) +
  geom_area(aes(group=station, fill=station), position="stack") +
  scale_x_date(name=NULL, expand=c(0,0)) +
  scale_y_continuous(name="# Mentions", label=scales::comma, limits=c(0, 8000), expand=c(0,0)) +
  ggthemes::scale_fill_tableau(name=NULL) +
  labs(title="Timeline") +
  theme_ipsum_rc(grid="XY") +
  theme(legend.position="bottom") +
  theme(axis.text.x=element_text(hjust=c(0, 0.5, 0.5, 0.5, 0.5, 0.5)))
```

The following is dynamically generated from the query results. View the R Markdown to see the code.

#### `r orange$top_matches$station[10]` / `r orange$top_matches$show[10]`

<`r orange$top_matches$preview_url[10]`>

`r htmltools::HTML(sprintf("<img src='%s'/>", orange$top_matches$preview_thumb[10]))`

>"`r gsub('trump', 'üçä', orange$top_matches$snippet[10], ignore.case=TRUE)`"

```{r warning=FALSE, message=FALSE, fig.height=9.5, fig.width=8, fig.retina=2}
from <- as.POSIXct("2017-09-08 18:00:00")
to <- as.POSIXct("2017-09-09 06:00:00")

tops <- top_trending_range(from, to)

rev(tail(sort(table(unlist(tops$overall_trending_phrases))), 20))
```


## Test Results

```{r message=FALSE, warning=FALSE, error=FALSE}
library(newsflash)
library(testthat)

date()

test_dir("tests/")
```

